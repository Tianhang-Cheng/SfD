# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/datamodules/00_omnidata.ipynb (unless otherwise specified).

# __all__ = ['OmnidataDataModule', 'show_batch_images', 'get_batch_cam_params', 'unproject_to_pointclouds',
#            'batch_unproject_to_multiview_pointclouds', 'subsample_multivew_pcs_batch', 'subsample_multivew_pcs',
#            'show_batch_pc', 'show_batch_scene', 'show_pc', 'plotly_mesh_kwargs', 'plotly_pc_kwargs']

from .omnidata_dataset import OmnidataDataset
from .component_datasets.taskonomy import TaskonomyDataset
from .component_datasets.replica import ReplicaDataset
from .component_datasets.replica_gso import GSOReplicaDataset
from .component_datasets.hypersim import HypersimDataset
from .component_datasets.blended_mvg import BlendedMVGDataset

from .pytorch_lightning_datamodule import OmnidataDataModule



# # Cell
# from fastcore.basics import store_attr, ifnone
# from pytorch_lightning import seed_everything
# from pytorch_lightning import LightningDataModule, LightningModule, Trainer
# from pytorch_lightning.trainer.supporters import CombinedLoader
# from torch.utils.data import DataLoader, ConcatDataset
# from torch.utils.data.sampler import BatchSampler, RandomSampler, SequentialSampler, WeightedRandomSampler
# import logging, loguru
# class OmnidataDataModule(LightningDataModule):
#     def __init__(self,
#                  tasks = ('rgb', 'normal', 'mask_valid'),
#                  train_datasets_to_options: Optional[Dict] = None,
#                  eval_datasets_to_options:  Optional[Dict] = None,
#                  shared_options:            Optional[Dict] = None,
#                  train_options:             Optional[Dict] = None,
#                  eval_options:              Optional[Dict] = None,
#                  dataloader_kwargs      = None,
#                  logger: logging.Logger = None
#                 ):
#         super().__init__()
#         store_attr()
#         self.logger = ifnone(logger, loguru.logger)
#         if 'batch_size' not in self.dataloader_kwargs: self.dataloader_kwargs['batch_size'] = 1
#         self.drop_last = dataloader_kwargs.pop('drop_last', False)
#         self._setup_datasets()

#     @staticmethod
#     def load_config(config_file):
#         with open(config_file, 'r') as stream:
#             config = yaml.safe_load(stream)
#         return config

#     def _setup_datasets(self, stage=None):
#         # Some options are unique to the train set
#         self.trainsets = {}
#         for dataset_name, dataset_opts in self.train_datasets_to_options.items():
#             opts_dict = {
#                 'split':'train',
#                 'tasks': self.tasks,
#                 **self.shared_options,
#                 **dataset_opts,
#                 **self.train_options
#             }
#             self.trainsets[dataset_name] =  eval(dataset_name)(eval(dataset_name).Options(**opts_dict))
#             self.logger.info(f'Train set ({dataset_name}) contains {len(self.trainsets[dataset_name])} samples.')
#         self.logger.success('Finished loading training sets.')

#         # Sompe options are unique to the val set
#         self.val_dataset_names, self.valsets = [], []
#         for dataset_name, dataset_opts in self.eval_datasets_to_options.items():
#             opts_dict = {
#                 'split':'val',
#                 'tasks': self.tasks,
#                 **self.shared_options,
#                 **dataset_opts,
#                 **self.eval_options,
#             }
#             self.valsets.append(eval(dataset_name)(eval(dataset_name).Options(**opts_dict)))
#             self.val_dataset_names.append(dataset_name)
#             self.logger.info(f'Val set ({dataset_name}) contains {len(self.valsets[-1])} samples.')
#         self.logger.success('Loaded validation sets.')


#     def train_dataloader(self):
#         # Train dataloader ensures each of the k datasets makes up 1/k of the batch
#         trainsets = self.trainsets.values()
#         trainsets_counts = [len(trainset) for trainset in trainsets]

#         dataset_sample_count = torch.tensor(trainsets_counts)
#         weights = 1. / dataset_sample_count.float()
#         samples_weight = []
#         for w, count in zip(weights, dataset_sample_count):
#             samples_weight += [w] * count
#         samples_weight = torch.tensor(samples_weight)

#         sampler = WeightedRandomSampler(samples_weight, len(samples_weight))
#         train_dataset = ConcatDataset(trainsets)
#         return DataLoader(
#             train_dataset,
#             sampler=sampler,
#             drop_last=self.drop_last,
#             **self.dataloader_kwargs
#         )

#     def val_dataloader(self):
#         return [
#             DataLoader(
#                 valset,
#                 drop_last=False,
#                 shuffle=True,
#                 **self.dataloader_kwargs
#             )
#             for valset in self.valsets
#         ]




# # Cell
# def show_batch_images(batch, batch_idx, view_idxs=None, keys=('rgb', 'depth_euclidean'), figsize=None):
#     if 'positive' in batch: batch = batch['positive']
#     if view_idxs is None: view_idxs = list(range(batch[keys[0]].shape[1]))
#     max_view_idx = max(view_idxs)
#     for k in keys:
#         n_batch, n_view = batch[k].shape[:2]
#         if n_batch <= batch_idx: raise ValueError(f"Trying to show batch_idx {batch_idx} but batch key {k} is of shape {batch[k].shape}")
#         if n_view <= max_view_idx: raise ValueError(f"Trying to show view number {max_view_idx} but batch key {k} is of shape {batch[k].shape}")
#     n_rows = len(keys)
#     n_cols = len(view_idxs)
#     cur_idx = 0
#     figsize = ifnone(figsize, (4*n_cols, 4*n_rows))
#     plt.figure(figsize=figsize)
#     for row, key in enumerate(keys):
#         for col, view_idx in enumerate(view_idxs):
#             cur_idx += 1
#             plt.subplot(n_rows, n_cols, cur_idx);
#             if row == 0: plt.title(f'View {view_idx}')
#             plt.imshow(batch[key][batch_idx][view_idx].permute([1,2,0]));
#     plt.tight_layout()
#     plt.show()


# # Cell
# import copy, math, torch
# from   torch import Tensor
# from   pytorch3d.structures import Meshes, Pointclouds
# from   pytorch3d.structures.pointclouds import join_pointclouds_as_batch
# from   pytorch3d.common.datatypes import Device
# from   src.datamodules.omnidata.pytorch3d_utils import GenericPinholeCamera, create_grid_ndc


# def get_batch_cam_params(batch):
#     if 'positive' in batch: raise ValueError('batch has key "positive"--just pass in each components please')
#     return_dict = {}
#     n_views = len(batch['point_info'])
#     for k in ['cam_to_world_R', 'cam_to_world_T', 'proj_K', 'proj_K_inv']:
#         return_dict[k] = torch.stack([batch['point_info'][i][k] for i in range(n_views)], dim=1)
#     return return_dict

# def unproject_to_pointclouds(
#     cam_to_world_R: Tensor,  # [V, 3, 3]
#     cam_to_world_T: Tensor,  # [V, 3]
#     proj_K:         Tensor,  # [V, 4, 4]
#     proj_K_inv:     Tensor,  # [V, 3, 3]
#     distance:       Tensor,  # [V, H, W]
#     features:       Tensor,  # [V, C, H, W]
#     mask_valid:     Optional[Tensor]=None,  # [V, H, W]
# ):
#     assert cam_to_world_R.ndim == 3, f'{cam_to_world_R.shape}'
#     assert cam_to_world_T.ndim == 2, f'{cam_to_world_T.shape}'
#     assert proj_K.ndim == 3,         f'{proj_K.shape}'
#     assert proj_K_inv.ndim == 3,     f'{proj_K_inv.shape}'
#     assert distance.ndim == 3,   f'{distance.shape}'
#     assert features.ndim == 4,   f'{features.shape}'
#     assert mask_valid.ndim == 3, f'{mask_valid.shape}'
#     features     = features.permute(0,2,3,1)
#     cameras      = GenericPinholeCamera(R=cam_to_world_R, T=cam_to_world_T, K=proj_K, K_inv=proj_K_inv, device=distance.device)
#     world_points = cameras.unproject_metric_depth_euclidean(distance, world_coordinates=True).reshape(len(cameras), -1, 3)
#     if mask_valid is None: return Pointclouds(points=[world_points], features=[features])
#     points = [feats[keep] for feats, keep in zip(features, mask_valid)]
#     # for pts in points: print(pts.shape)
#     return Pointclouds(
#         points   = [pts[keep.reshape(-1)]   for pts, keep in zip(world_points, mask_valid)],
#         features = [feats[keep] for feats, keep in zip(features, mask_valid)]
#     )

# def batch_unproject_to_multiview_pointclouds(
#     cam_to_world_R: Tensor,
#     cam_to_world_T: Tensor,
#     proj_K:         Tensor,
#     proj_K_inv:     Tensor,
#     distance:       Tensor,
#     features:       Tensor,
#     mask_valid:     Optional[Tensor]=None,
# ):
#     if mask_valid is None: mask_valid = [None] * len(cam_to_world_R)
#     return [
#         unproject_to_pointclouds(
#             cam_to_world_R = _cam_to_world_R,
#             cam_to_world_T = _cam_to_world_T,
#             proj_K         = _proj_K,
#             proj_K_inv     = _proj_K_inv,
#             distance       = _distance,
#             features       = _features,
#             mask_valid     = _mask_valid,
#         )
#         for (_cam_to_world_R, _cam_to_world_T, _proj_K, _proj_K_inv, _distance, _features, _mask_valid) \
#         in zip(cam_to_world_R, cam_to_world_T, proj_K, proj_K_inv, distance, features, mask_valid)
#     ]

# #export
# def subsample_multivew_pcs_batch(pcs_batch, max_per_view=-1):
#     '''
#         Given a list of pointcloud lists (all of same length): [pc1], [pc2], ..., [pcN]
#         Subsamples all pointclouds (matched by index)
#     '''
#     if max_per_view < 0: return pcs_batch
#     if any(len(pcs_batch[0])!= len(i) for i in pcs_batch): raise ValueError(f"Trying to subsamples matched pointclouds with differing batch sizes: {[len(i) for i in pcs_batch]}")
#     pcs_out = [subsample_multivew_pcs(pcs_all=pcs_example, max_per_view=max_per_view) for pcs_example in zip(*pcs_batch)]
#     return zip(*pcs_out)

# def subsample_multivew_pcs(pcs_all, max_per_view):
#     '''
#         Given a list of pointclouds pc1, pc2, ..., pcN
#         Subsamples all pointclouds (matched by index)
#     '''
#     pc_examplar = pcs_all[0]
#     indices = [torch.arange(l, device=pc_examplar.device).unsqueeze(-1) for l in pc_examplar.num_points_per_cloud()]
#     pc_idx  = Pointclouds(points=pc_examplar.points_list(), features=indices).subsample(max_per_view)
#     idxs_list = [idxs.squeeze(-1) for idxs in pc_idx.features_list()]
#     return [Pointclouds(
#             points=[p[idxs] for (p, idxs) in zip(pcs.points_list(), idxs_list)],
#             features=[f[idxs] for (f, idxs) in zip(pcs.features_list(), idxs_list)])
#         for pcs in pcs_all]

# # Cell
# from src.the_usual_suspects.libs import *

# from pytorch3d.vis.plotly_vis import AxisArgs, plot_batch_individually, plot_scene
# plotly_mesh_kwargs = dict(
#     xaxis={"backgroundcolor":"rgb(200, 200, 230)"},
#     yaxis={"backgroundcolor":"rgb(230, 200, 200)"},
#     zaxis={"backgroundcolor":"rgb(200, 230, 200)"},
#     axis_args=AxisArgs(showgrid=True)
# )
# plotly_pc_kwargs = dict(
#     xaxis={"backgroundcolor":"rgb(200, 200, 230)"},
#     yaxis={"backgroundcolor":"rgb(230, 200, 200)"},
#     zaxis={"backgroundcolor":"rgb(200, 230, 200)"},
#     axis_args=AxisArgs(showgrid=True),
#     scaleratio = 1, point_size=30,
# )


# def show_batch_pc(batch, batch_idx, view_idxs, view_kwargs=plotly_pc_kwargs, figsize=750):
#     if isinstance(view_idxs, int): view_idxs = [view_idxs]
#     pos        = batch.get('positive', batch)
#     bpv        = pos['building'][batch_idx], pos['point'][batch_idx], pos['view'][batch_idx]
#     dataset    = pos['dataset'][batch_idx]
#     mask_valid = pos['mask_valid'].bool()[batch_idx,view_idxs]#.squeeze(1)
#     distance   = pos['depth_euclidean'][batch_idx,view_idxs]#.squeeze(1)
#     rgb        = pos['rgb'][batch_idx,view_idxs].unsqueeze(1)
#     cam_params = { k: v[batch_idx,view_idxs].unsqueeze(1)
#                    for (k, v) in get_batch_cam_params(pos).items()}
#     pcs_full   = batch_unproject_to_multiview_pointclouds(mask_valid=mask_valid, distance=distance, features=rgb, **cam_params)
#     pcs_full   = join_pointclouds_as_batch(pcs_full)
#     fig        = plot_scene({ f"{dataset}: {bpv}": { "Joined": pcs_full, },}, **view_kwargs)
#     fig.update_layout(height=figsize, width=figsize)
#     fig.show()

# def show_batch_scene(batch, batch_idx, view_idxs, view_kwargs=plotly_pc_kwargs, figsize=750):
#     if isinstance(view_idxs, int): view_idxs = [view_idxs]
#     pos        = batch.get('positive', batch)
#     bpv        = pos['building'][batch_idx], pos['point'][batch_idx], pos['view'][batch_idx]
#     dataset    = pos['dataset'][batch_idx]
#     mask_valid = pos['mask_valid'].bool()[batch_idx,view_idxs]#.squeeze(1)
#     distance   = pos['depth_euclidean'][batch_idx,view_idxs]#.squeeze(1)
#     rgb        = pos['rgb'][batch_idx,view_idxs].unsqueeze(1)
#     cam_params = { k: v[batch_idx,view_idxs].unsqueeze(1)
#                    for (k, v) in get_batch_cam_params(pos).items()}
#     pcs_full   = batch_unproject_to_multiview_pointclouds(mask_valid=mask_valid, distance=distance, features=rgb, **cam_params)
#     cameras    = GenericPinholeCamera(
#                     R=cam_params['cam_to_world_R'].squeeze(1),
#                     T=cam_params['cam_to_world_T'].squeeze(1),
#                     K=cam_params['proj_K'].squeeze(1),
#                     K_inv=cam_params['proj_K_inv'].squeeze(1),
#                     device=distance.device)
#     pcs_full   = join_pointclouds_as_batch(pcs_full)
#     fig        = plot_scene({ f"{dataset}: {bpv}": {
#                                 "Points": pcs_full,
#                                 "cameras": cameras,
#                             },},
#                             **view_kwargs)
#     fig.update_layout(height=figsize, width=figsize)
#     fig.show()


# def show_pc(pc=None, points=None, colors=None, view_kwargs=plotly_pc_kwargs, figsize=750):
#     if pc is None: pc = Pointclouds(points=points, features=colors)
#     fig        = plot_scene({ "Merged pointcloud": { "Joined": pc, },}, **view_kwargs)
#     fig.update_layout(height=figsize, width=figsize)
#     fig.show()

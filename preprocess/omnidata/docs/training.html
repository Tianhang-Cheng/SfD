---

title: Training with Omnidata


keywords: fastai
sidebar: home_sidebar

summary: "This page describes how to train your own models with omnidata. For the exact code used in the paper, see the <a href='https://github.com/EPFL-VILAB/omnidata/tree/main/paper_code'>paper code dump repo</a>."
description: "This page describes how to train your own models with omnidata. For the exact code used in the paper, see the <a href='https://github.com/EPFL-VILAB/omnidata/tree/main/paper_code'>paper code dump repo</a>."
nb_path: "nbs/06_training.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/06_training.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Training-Strong-Models">Training Strong Models<a class="anchor-link" href="#Training-Strong-Models"> </a></h2><p>Omnidata is a means to train models in different vision tasks. Here, we provide the code for training our depth and surface normal estimation models. You can train the models with the following commands:</p>
<h2 id="Depth-Estimation">Depth Estimation<a class="anchor-link" href="#Depth-Estimation"> </a></h2><p>We train DPT-based models on Omnidata using 3 different losses: <code>scale- and shift-invariant loss</code> and <code>scale-invariant gradient matching term</code> introduced in <a href="https://arxiv.org/pdf/1907.01341v3.pdf">MiDaS</a>, and also <code>virtual normal loss</code> introduced <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Yin_Enforcing_Geometric_Constraints_of_Virtual_Normal_for_Depth_Prediction_ICCV_2019_paper.pdf">here</a>.</p>
<div class="highlight"><pre><span></span>python train_depth.py --config_file config/depth.yml --experiment_name rgb2depth --val_check_interval <span class="m">3000</span> --limit_val_batches <span class="m">100</span> --max_epochs <span class="m">10</span>
</pre></div>
<h4 id="MiDaS-Implementation">MiDaS Implementation<a class="anchor-link" href="#MiDaS-Implementation"> </a></h4><p>We provide an implementation of the MiDaS Loss, specifically the <code>ssimae (scale- and shift invariant MAE) loss</code> and the <code>scale-invariant gradient matching term</code> in <code>losses/midas_loss</code>.py. The video below shows the output of our MiDaS reimplementation (a DPT trained on the Omnidata starer datset) compared to the original DPT w/ MiDaS trained on a mix of 10 depth datasets that contains both real images and depth sensor readings. The resampled data from Omnidata seems not to hurt training, since the reimplemented version better captures the 3D shape (quantitative comparisons of depth estimation are in the paper).</p>
<video width="100%" height="464" playsinline="" autoplay="" center="" loop="" muted="" class="video-bg" id="video-bg" poster="./loading.gif">
<source src="/omnidata-tools/images/torch/depth/depth_to_norm.mp4" type="video/mp4" alt="HTML5 background video">
</video><p>MiDaS loss is useful for training depth estimation models on mixed datasets with different depth ranges and scales, similar to our dataset. An example usage is shown below:</p>
<div class="highlight"><pre><span></span>from losses.midas_loss import MidasLoss
<span class="nv">midas_loss</span> <span class="o">=</span> MidasLoss<span class="o">(</span><span class="nv">alpha</span><span class="o">=</span><span class="m">0</span>.1<span class="o">)</span>
midas_loss, ssi_mae_loss, <span class="nv">reg_loss</span> <span class="o">=</span> midas_loss<span class="o">(</span>depth_prediction, depth_gt, mask<span class="o">)</span>
</pre></div>
<p><code>alpha</code> specifies the weight of the <code>gradient matching term</code> in the total loss, and <code>mask</code> indicates the valid pixels of the image.</p>
<table>
<thead><tr>
<th style="text-align:center"></th>
<th style="text-align:center"></th>
<th style="text-align:center"></th>
<th style="text-align:center"></th>
<th style="text-align:center"></th>
<th style="text-align:center"></th>
<th style="text-align:center"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/omnidata-tools/images/torch/depth/240_rgb.png" style='max-width: 100%;'/></td>
<td style="text-align:center"><img src="/omnidata-tools/images/torch/depth/64_rgb.png" style='max-width: 100%;'/></td>
<td style="text-align:center"><img src="/omnidata-tools/images/torch/depth/124_rgb.png" style='max-width: 100%;'/></td>
<td style="text-align:center"><img src="/omnidata-tools/images/torch/depth/106_rgb.png" style='max-width: 100%;'/></td>
<td style="text-align:center"><img src="/omnidata-tools/images/torch/depth/62_rgb.png" style='max-width: 100%;'/></td>
<td style="text-align:center"><img src="/omnidata-tools/images/torch/depth/184_rgb.png" style='max-width: 100%;'/></td>
<td style="text-align:center"><img src="/omnidata-tools/images/torch/depth/192_rgb.png" style='max-width: 100%;'/></td>
</tr>
<tr>
<td style="text-align:center"><img src="/omnidata-tools/images/torch/depth/240_depth.png" style='max-width: 100%;'/></td>
<td style="text-align:center"><img src="/omnidata-tools/images/torch/depth/64_depth.png" style='max-width: 100%;'/></td>
<td style="text-align:center"><img src="/omnidata-tools/images/torch/depth/124_depth.png" style='max-width: 100%;'/></td>
<td style="text-align:center"><img src="/omnidata-tools/images/torch/depth/106_depth.png" style='max-width: 100%;'/></td>
<td style="text-align:center"><img src="/omnidata-tools/images/torch/depth/62_depth.png" style='max-width: 100%;'/></td>
<td style="text-align:center"><img src="/omnidata-tools/images/torch/depth/184_depth.png" style='max-width: 100%;'/></td>
<td style="text-align:center"><img src="/omnidata-tools/images/torch/depth/192_depth.png" style='max-width: 100%;'/></td>
</tr>
</tbody>
</table>
<h2 id="Surface-Normal-Estimation">Surface Normal Estimation<a class="anchor-link" href="#Surface-Normal-Estimation"> </a></h2><p>We train a <a href="https://arxiv.org/pdf/1505.04597.pdf">UNet</a> architecture (6 down/6 up) for surface normal estimation using <code>L1 Loss</code> and <code>Cosine Angular Loss</code>.</p>
<div class="highlight"><pre><span></span>python train_normal.py --config_file config/normal.yml --experiment_name rgb2normal --val_check_interval <span class="m">3000</span> --limit_val_batches <span class="m">100</span> --max_epochs <span class="m">10</span>
</pre></div>
<p>Here are some results (compared to X-Task Consistency):</p>
<video width="100%" height="464" playsinline="" autoplay="" center="" loop="" muted="" class="video-bg" id="video-bg" poster="./loading.gif">
<source src="https://omnidata.vision/assets/videos/surface_normal1.mp4" type="video/mp4" alt="HTML5 background video">
</video>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="3D-Depth-of-Field-Augmentation">3D Depth-of-Field Augmentation<a class="anchor-link" href="#3D-Depth-of-Field-Augmentation"> </a></h2><p>Mid-level cues can be used for data augmentations in addition to training targets. The availability of full scene geometry in our dataset makes the possibility of doing Image Refocusing as a 3D data augmentation. You can find an implementation of this augmentation in <code>data/refocus_augmentation.py</code>. You can run this augmentation on some sample images from our dataset with the following command.</p>
<div class="highlight"><pre><span></span>python demo_refocus.py --input_path assets/demo_refocus/ --output_path assets/demo_refocus
</pre></div>
<p>This will refocus RGB images by blurring them according to <code>depth_euclidean</code> for each image. You can specify some parameters of the augmentation with the following tags: <code>--num_quantiles</code> (number of qualtiles to use in blur stack), <code>--min_aperture</code> (smallest aperture to use), <code>--max_aperture</code> (largest aperture to use). Aperture size is selected log-uniformly in the range between min and max aperture.</p>
<table>
<thead><tr>
<th style="text-align:center">Shallow Focus</th>
<th style="text-align:center">Mid Focus</th>
<th style="text-align:center">Far Focus</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><img src="/omnidata-tools/images/torch/demo_refocus/IMG_4642.gif" style='max-width: 100%;'/></td>
<td style="text-align:center"><img src="/omnidata-tools/images/torch/demo_refocus/IMG_4644.gif" style='max-width: 100%;'/></td>
<td style="text-align:center"><img src="/omnidata-tools/images/torch/demo_refocus/IMG_4643.gif" style='max-width: 100%;'/></td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Citation">Citation<a class="anchor-link" href="#Citation"> </a></h2><p>If you find the code or models useful, please cite our paper:</p>

<pre><code>@inproceedings{eftekhar2021omnidata,
  title={Omnidata: A Scalable Pipeline for Making Multi-Task Mid-Level Vision Datasets From 3D Scans},
  author={Eftekhar, Ainaz and Sax, Alexander and Malik, Jitendra and Zamir, Amir},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={10786--10796},
  year={2021}
}</code></pre>

</div>
</div>
</div>
</div>


